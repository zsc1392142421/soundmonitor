{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集：https://serv.cusp.nyu.edu/projects/urbansounddataset/  \n",
    "\n",
    "librosa：https://github.com/librosa/librosa  \n",
    "\n",
    "分类：  \n",
    "0 = air_conditioner  \n",
    "1 = car_horn  \n",
    "2 = children_playing  \n",
    "3 = dog_bark  \n",
    "4 = drilling  \n",
    "5 = engine_idling  \n",
    "6 = gun_shot  \n",
    "7 = jackhammer  \n",
    "8 = siren  \n",
    "9 = street_music  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import librosa # pip install librosa\n",
    "from tqdm import tqdm # pip install tqdm\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCsv(path):\n",
    "    with open(path,'r') as f:\n",
    "        infoList=[]\n",
    "        \n",
    "        #读取到的是个对象\n",
    "        allFileInfo=csv.reader(f)\n",
    "\n",
    "        #读取每一行\n",
    "        for row in allFileInfo:\n",
    "            infoList.append(row)\n",
    "\n",
    "        return infoList\n",
    "\n",
    "path=r'C:/Users/zsc/Desktop/train_noisy_油田.csv'\n",
    "info=readCsv(path)[1:]\n",
    "info = info[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 找出个数最多的20个类并数字化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeldict = {}\n",
    "for each in info:\n",
    "    value = each[1]\n",
    "    if value not in labeldict.keys():\n",
    "        labeldict[value] = [each[0]]\n",
    "    else:\n",
    "        labeldict[value].append(each[0])\n",
    "        \n",
    "temp = list(labeldict.keys())\n",
    "lengths = [len(each) for each in labeldict.values()]\n",
    "sort_lengths = sorted(enumerate(lengths),key = lambda x:x[1])\n",
    "max_20 = sort_lengths[-20:]\n",
    "\n",
    "indexlist = []\n",
    "for each in max_20:\n",
    "    indexlist.append(each[0])\n",
    "    \n",
    "max_20_label = []\n",
    "for each in indexlist:\n",
    "    max_20_label.append(temp[each])\n",
    "    \n",
    "max_20_label_dict = {}\n",
    "for index,value in enumerate(max_20_label):\n",
    "    max_20_label_dict[value] = index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "# validation数据集占比\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .2,\n",
    "                      \"Percentage of the training data to use for validation\")\n",
    "# 父目录\n",
    "tf.flags.DEFINE_string(\n",
    "    \"parent_dir\", 'D:/audio library/', \"Data source for the data.\")\n",
    "# 子目录\n",
    "tf.flags.DEFINE_list(\"tr_sub_dirs\", ['debug_initial/'], \"Data source for the data.\")\n",
    "# Model Hyperparameters\n",
    "# 第一层输入，MFCC信号\n",
    "tf.flags.DEFINE_integer(\"n_inputs\", 32, \"Number of MFCCs (default: 40)\")\n",
    "# cell个数\n",
    "tf.flags.DEFINE_integer(\"n_hidden\", 300, \"Number of cells (default: 300)\")\n",
    "# 分类数\n",
    "tf.flags.DEFINE_integer(\"n_classes\", 16, \"Number of classes (default: 10)\")\n",
    "# 学习率\n",
    "tf.flags.DEFINE_float(\"lr\", 0.005, \"Learning rate (default: 0.005)\")\n",
    "# dropout参数\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5,\n",
    "                      \"Dropout keep probability (default: 0.5)\")\n",
    "\n",
    "# Training parameters\n",
    "# 批次大小\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 50, \"Batch Size (default: 50)\")\n",
    "# 迭代周期\n",
    "tf.flags.DEFINE_integer(\n",
    "    \"num_epochs\", 100, \"Number of training epochs (default: 100)\")\n",
    "# 多少step测试一次\n",
    "tf.flags.DEFINE_integer(\n",
    "    \"evaluate_every\", 50, \"Evaluate model on dev set after this many steps (default: 50)\")\n",
    "# 多少step保存一次模型\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 500,\n",
    "                        \"Save model after this many steps (default: 500)\")\n",
    "# 最多保存多少个模型\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 2,\n",
    "                        \"Number of checkpoints to store (default: 2)\")\n",
    "\n",
    "# flags解析\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS.flag_values_dict()\n",
    "\n",
    "# 打印所有参数\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得训练用的wav文件路径列表  \n",
    "def get_wav_files(parent_dir,sub_dirs): \n",
    "    wav_files = []  \n",
    "    for l, sub_dir in enumerate(sub_dirs):\n",
    "        wav_path = os.path.join(parent_dir, sub_dir)\n",
    "        for (dirpath, dirnames, filenames) in os.walk(wav_path):  \n",
    "            for filename in filenames:\n",
    "                if filename.endswith('.wav') or filename.endswith('.WAV'):  \n",
    "                    filename_path = os.path.join(dirpath, filename)\n",
    "                    wav_files.append(filename_path)  \n",
    "    return wav_files,wav_path\n",
    "\n",
    "# 获取文件mfcc特征和对应标签\n",
    "def extract_features(wav_path,max_20_label_dict):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    wav_files = []\n",
    "\n",
    "    labelpath = r'C:\\Users\\zsc\\Desktop\\train_label _油田.csv'\n",
    "    people_label = readCsv(labelpath)\n",
    "\n",
    "    for each in people_label:\n",
    "        filename_path = os.path.join(wav_path, each[0])\n",
    "        wav_files.append(filename_path)\n",
    "        labels.append(max_20_label_dict[each[1]])\n",
    "\n",
    "    for wav_file in tqdm(wav_files):\n",
    "            # 读入音频文件\n",
    "            audio,fs = librosa.load(wav_file)\n",
    "\n",
    "            # 获取音频mfcc特征\n",
    "            # [n_steps, n_inputs]\n",
    "            mfccs = np.transpose(librosa.feature.mfcc(y=audio, sr=fs, n_mfcc=FLAGS.n_inputs), [1,0]) \n",
    "            inputs.append(mfccs.tolist()) \n",
    "    return inputs, np.array(labels, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得训练用的wav文件路径列表\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "# wav_files, wav_path = get_wav_files(FLAGS.parent_dir, FLAGS.tr_sub_dirs)\n",
    "# # 获取文件mfcc特征和对应标签\n",
    "# tr_features, tr_labels = extract_features(wav_path, max_20_label_dict)\n",
    "\n",
    "# np.save('tr_features_32.npy', tr_features)\n",
    "# np.save('tr_labels_32.npy', tr_labels)\n",
    "\n",
    "tr_features = np.load('tr_features_32.npy')\n",
    "tr_labels = np.load('tr_labels_32.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len: 259\n"
     ]
    }
   ],
   "source": [
    "#(batch,step,input)\n",
    "#(50,173,40)\n",
    "\n",
    "# 计算最长的step\n",
    "wav_max_len = max([len(feature) for feature in tr_features])\n",
    "print(\"max_len:\",wav_max_len)\n",
    "\n",
    "# 填充0\n",
    "tr_data = []\n",
    "for mfccs in tr_features:  \n",
    "    while len(mfccs) < wav_max_len: #只要小于wav_max_len就补n_inputs个0\n",
    "        mfccs.append([0] * FLAGS.n_inputs) \n",
    "    tr_data.append(mfccs)\n",
    "\n",
    "tr_data = np.array(tr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(tr_data)))\n",
    "x_shuffled = tr_data[shuffle_indices]\n",
    "y_shuffled = tr_labels[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "# 数据集切分为两部分\n",
    "dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y_shuffled)))\n",
    "train_x, test_x = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "train_y, test_y = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\zsc\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-9-28fe1bc70dde>:16: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-9-28fe1bc70dde>:20: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-9-28fe1bc70dde>:23: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\zsc\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-9-28fe1bc70dde>:32: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# placeholder\n",
    "x = tf.placeholder(\"float\", [None, wav_max_len, FLAGS.n_inputs])\n",
    "y = tf.placeholder(\"float\", [None])\n",
    "dropout = tf.placeholder(tf.float32)\n",
    "# learning rate\n",
    "lr = tf.Variable(FLAGS.lr, dtype=tf.float32, trainable=False)\n",
    "\n",
    "# 定义RNN网络\n",
    "# 初始化权制和偏置\n",
    "weights = tf.Variable(tf.truncated_normal([FLAGS.n_hidden, FLAGS.n_classes], stddev=0.1))\n",
    "biases = tf.Variable(tf.constant(0.1, shape=[FLAGS.n_classes]))\n",
    "\n",
    "# 多层网络\n",
    "num_layers = 2\n",
    "def grucell():\n",
    "    cell = tf.contrib.rnn.GRUCell(FLAGS.n_hidden)\n",
    "#     cell = tf.contrib.rnn.LSTMCell(FLAGS.n_hidden)\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=dropout)\n",
    "    return cell\n",
    "cell = tf.contrib.rnn.MultiRNNCell([grucell() for _ in range(num_layers)])\n",
    "\n",
    "\n",
    "outputs,final_state = tf.nn.dynamic_rnn(cell,x,dtype=tf.float32)\n",
    "\n",
    "# 预测值\n",
    "prediction = tf.nn.softmax(tf.matmul(final_state[0],weights) + biases)\n",
    "\n",
    "# labels转one_hot格式\n",
    "one_hot_labels = tf.one_hot(indices=tf.cast(y, tf.int32), depth=FLAGS.n_classes)\n",
    "\n",
    "# loss\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=one_hot_labels))\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(cross_entropy)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction,1), tf.argmax(one_hot_labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "        Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    # 每个epoch的num_batch\n",
    "    num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n",
    "    print(\"num_batches_per_epoch:\",num_batches_per_epoch)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches_per_epoch: 34\n",
      "Iter 50, loss 2.33938, tr_acc 0.53471, ts_acc 0.54481, lr 0.00495\n",
      "Iter 100, loss 2.21875, tr_acc 0.67647, ts_acc 0.64858, lr 0.00490\n",
      "Iter 150, loss 2.08778, tr_acc 0.79882, ts_acc 0.75943, lr 0.00485\n",
      "Iter 200, loss 2.06992, tr_acc 0.82000, ts_acc 0.77358, lr 0.00480\n",
      "Iter 250, loss 2.04733, tr_acc 0.84824, ts_acc 0.79717, lr 0.00475\n",
      "Iter 300, loss 2.03830, tr_acc 0.84471, ts_acc 0.79717, lr 0.00471\n",
      "Iter 350, loss 2.02721, tr_acc 0.85765, ts_acc 0.79717, lr 0.00466\n",
      "Iter 400, loss 2.02030, tr_acc 0.86353, ts_acc 0.80189, lr 0.00461\n",
      "Iter 450, loss 2.01384, tr_acc 0.86706, ts_acc 0.80425, lr 0.00457\n",
      "Iter 500, loss 2.01088, tr_acc 0.86941, ts_acc 0.80425, lr 0.00452\n",
      "Saved model checkpoint to sounds_models/model-500\n",
      "\n",
      "Iter 550, loss 2.00720, tr_acc 0.86941, ts_acc 0.81132, lr 0.00448\n",
      "Iter 600, loss 2.00396, tr_acc 0.87059, ts_acc 0.81604, lr 0.00443\n",
      "Iter 650, loss 2.00171, tr_acc 0.87118, ts_acc 0.81368, lr 0.00439\n",
      "Iter 700, loss 1.98920, tr_acc 0.88765, ts_acc 0.84906, lr 0.00434\n",
      "Iter 750, loss 1.97669, tr_acc 0.91412, ts_acc 0.88208, lr 0.00430\n",
      "Iter 800, loss 1.96311, tr_acc 0.91941, ts_acc 0.90330, lr 0.00426\n",
      "Iter 850, loss 1.96860, tr_acc 0.91529, ts_acc 0.89858, lr 0.00421\n",
      "Iter 900, loss 1.96260, tr_acc 0.91882, ts_acc 0.89387, lr 0.00417\n",
      "Iter 950, loss 1.92622, tr_acc 0.95765, ts_acc 0.92689, lr 0.00413\n",
      "Iter 1000, loss 1.92477, tr_acc 0.95706, ts_acc 0.92925, lr 0.00409\n",
      "Saved model checkpoint to sounds_models/model-1000\n",
      "\n",
      "Iter 1050, loss 1.92235, tr_acc 0.95765, ts_acc 0.91981, lr 0.00405\n",
      "Iter 1100, loss 1.91875, tr_acc 0.96824, ts_acc 0.92925, lr 0.00401\n",
      "Iter 1150, loss 1.91221, tr_acc 0.97118, ts_acc 0.93396, lr 0.00397\n",
      "Iter 1200, loss 1.90368, tr_acc 0.97588, ts_acc 0.94104, lr 0.00393\n",
      "Iter 1250, loss 1.90032, tr_acc 0.97588, ts_acc 0.94575, lr 0.00389\n",
      "Iter 1300, loss 1.90302, tr_acc 0.97588, ts_acc 0.93868, lr 0.00385\n",
      "Iter 1350, loss 1.89796, tr_acc 0.97765, ts_acc 0.94811, lr 0.00381\n",
      "Iter 1400, loss 1.89537, tr_acc 0.98000, ts_acc 0.95047, lr 0.00377\n",
      "Iter 1450, loss 1.89488, tr_acc 0.98059, ts_acc 0.95047, lr 0.00374\n",
      "Iter 1500, loss 1.89815, tr_acc 0.97882, ts_acc 0.94104, lr 0.00370\n",
      "Saved model checkpoint to sounds_models/model-1500\n",
      "\n",
      "Iter 1550, loss 1.89396, tr_acc 0.98176, ts_acc 0.94811, lr 0.00366\n",
      "Iter 1600, loss 1.89427, tr_acc 0.98235, ts_acc 0.93868, lr 0.00362\n",
      "Iter 1650, loss 1.89254, tr_acc 0.98235, ts_acc 0.94340, lr 0.00359\n",
      "Iter 1700, loss 1.89148, tr_acc 0.98353, ts_acc 0.94340, lr 0.00355\n",
      "Iter 1750, loss 1.88973, tr_acc 0.98529, ts_acc 0.94575, lr 0.00352\n",
      "Iter 1800, loss 1.88903, tr_acc 0.98588, ts_acc 0.94575, lr 0.00348\n",
      "Iter 1850, loss 1.88864, tr_acc 0.98588, ts_acc 0.94575, lr 0.00345\n",
      "Iter 1900, loss 1.88857, tr_acc 0.98588, ts_acc 0.94575, lr 0.00341\n",
      "Iter 1950, loss 1.88849, tr_acc 0.98588, ts_acc 0.95047, lr 0.00338\n",
      "Iter 2000, loss 1.87777, tr_acc 0.99706, ts_acc 0.96934, lr 0.00334\n",
      "Saved model checkpoint to sounds_models/model-2000\n",
      "\n",
      "Iter 2050, loss 1.87768, tr_acc 0.99706, ts_acc 0.96934, lr 0.00331\n",
      "Iter 2100, loss 1.87766, tr_acc 0.99706, ts_acc 0.96934, lr 0.00328\n",
      "Iter 2150, loss 1.87765, tr_acc 0.99706, ts_acc 0.96934, lr 0.00325\n",
      "Iter 2200, loss 1.87763, tr_acc 0.99706, ts_acc 0.96934, lr 0.00321\n",
      "Iter 2250, loss 1.87761, tr_acc 0.99706, ts_acc 0.96934, lr 0.00318\n",
      "Iter 2300, loss 1.87757, tr_acc 0.99706, ts_acc 0.96934, lr 0.00315\n",
      "Iter 2350, loss 1.87728, tr_acc 0.99765, ts_acc 0.96698, lr 0.00312\n",
      "Iter 2400, loss 1.87746, tr_acc 0.99706, ts_acc 0.96934, lr 0.00309\n",
      "Iter 2450, loss 1.87721, tr_acc 0.99765, ts_acc 0.97642, lr 0.00306\n",
      "Iter 2500, loss 1.87717, tr_acc 0.99765, ts_acc 0.97406, lr 0.00303\n",
      "Saved model checkpoint to sounds_models/model-2500\n",
      "\n",
      "Iter 2550, loss 1.87709, tr_acc 0.99765, ts_acc 0.97642, lr 0.00299\n",
      "Iter 2600, loss 1.87705, tr_acc 0.99765, ts_acc 0.97642, lr 0.00296\n",
      "Iter 2650, loss 1.87703, tr_acc 0.99765, ts_acc 0.97877, lr 0.00294\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-3e559be8be27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout_keep_prob\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# 测试\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# 定义saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) \n",
    "\n",
    "    # Generate batches\n",
    "    batches = batch_iter(list(zip(train_x, train_y)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "\n",
    "    for i,batch in enumerate(batches):\n",
    "        i = i + 1\n",
    "        x_batch, y_batch = zip(*batch)\n",
    "        sess.run([optimizer], feed_dict={x: x_batch, y: y_batch, dropout: FLAGS.dropout_keep_prob})\n",
    "        \n",
    "        # 测试\n",
    "        if i % FLAGS.evaluate_every == 0:\n",
    "            sess.run(tf.assign(lr, FLAGS.lr * (0.99 ** (i // FLAGS.evaluate_every))))\n",
    "            learning_rate = sess.run(lr)\n",
    "            tr_acc, _loss = sess.run([accuracy, cross_entropy], feed_dict={x: train_x, y: train_y, dropout: 1.0})\n",
    "            ts_acc = sess.run(accuracy, feed_dict={x: test_x, y: test_y, dropout: 1.0})\n",
    "            print(\"Iter {}, loss {:.5f}, tr_acc {:.5f}, ts_acc {:.5f}, lr {:.5f}\".format(i, _loss, tr_acc, ts_acc, learning_rate))\n",
    "\n",
    "        # 保存模型\n",
    "        if i % FLAGS.checkpoint_every == 0:\n",
    "            path = saver.save(sess, \"sounds_models/model\", global_step=i)\n",
    "            print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
